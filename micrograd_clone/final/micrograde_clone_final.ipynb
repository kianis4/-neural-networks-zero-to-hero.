{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label='') -> None:\n",
    "        # Initialize the Value object with data, gradient, and other properties\n",
    "        self.data = data  # The actual data value\n",
    "        self.grad = 0.0  # The gradient of this value, initialized to 0\n",
    "        # A function to compute the gradient, does nothing by default for leaf nodes\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)  # Set of child nodes in the computation graph\n",
    "        self._op = _op  # The operation that created this node\n",
    "        self.label = label  # Optional label for the node\n",
    "\n",
    "    def __repr__(self):\n",
    "        # String representation of the Value object\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        # Addition operation between two Value objects\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')  # Create a new Value for the result\n",
    "        def _backward():\n",
    "            # Backward pass for addition, distribute the gradient to both operands\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward  # Set the backward function for the result\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # Right-side addition, reuses the __add__ method\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):\n",
    "        # Negation operation, implemented as multiplication by -1\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        # Subtraction operation, implemented as addition of a negation\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        # Multiplication operation between two Value objects\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')  # Create a new Value for the result\n",
    "        def _backward():\n",
    "            # Backward pass for multiplication, apply the product rule\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward  # Set the backward function for the result\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # Right-side multiplication, reuses the __mul__ method\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        # Division operation, implemented as multiplication by the reciprocal\n",
    "        return self * other**-1\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        # Power operation, only supports int/float exponents\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')  # Create a new Value for the result\n",
    "        def _backward():\n",
    "            # Backward pass for power, apply the power rule\n",
    "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward  # Set the backward function for the result\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        # Hyperbolic tangent activation function\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)  # Compute tanh\n",
    "        out = Value(t, (self,), 'tanh')  # Create a new Value for the result\n",
    "        def _backward():\n",
    "            # Backward pass for tanh, derivative is 1 - tanh^2\n",
    "            self.grad += (1 - t**2) * out.grad\n",
    "        out._backward = _backward  # Set the backward function for the result\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        # Exponential function\n",
    "        x = self.data\n",
    "        out = Value(math.exp(x), (self,), 'exp')  # Create a new Value for the result\n",
    "        def _backward():\n",
    "            # Backward pass for exp, derivative is exp itself\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward  # Set the backward function for the result\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        # Perform backpropagation to compute gradients\n",
    "        topo = []  # List to store the topologically sorted nodes\n",
    "        visited = set()  # Set to keep track of visited nodes\n",
    "\n",
    "        def build_topo(v):\n",
    "            # Helper function to build the topological order\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)  # Start building the topological order from this node\n",
    "        self.grad = 1.0  # Initialize the gradient of the output node to 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()  # Apply the chain rule in reverse topological order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    # Initialize a neuron with a given number of inputs (nin)\n",
    "    def __init__(self, nin) -> None:\n",
    "        # Initialize weights (w) randomly between -1 and 1 for each input\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "        # Initialize the bias (b) randomly between -1 and 1\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "    \n",
    "    # Define the forward pass of the neuron\n",
    "    def __call__(self, x):\n",
    "        # Compute the weighted sum of inputs and bias (w * x + b)\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # Apply the tanh activation function to the weighted sum\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    # Gather all parameters (weights and bias) of the neuron\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a layer of neurons\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        # Initialize the layer with a list of neurons\n",
    "        # Each neuron in the layer will have 'nin' inputs\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Perform the forward pass for each neuron in the layer\n",
    "        # 'x' is the input to the layer\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        # If there is only one neuron in the layer, return its output directly\n",
    "        # Otherwise, return the list of outputs from all neurons\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Gather all parameters (weights and biases) from all neurons in the layer\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # Initialize the MLP with the number of inputs and the number of neurons in each layer\n",
    "    def __init__(self, nin, nouts):\n",
    "        # Create a list of sizes, starting with the number of inputs followed by the number of neurons in each layer\n",
    "        sz = [nin] + nouts\n",
    "        # Initialize the layers of the MLP\n",
    "        # Each layer is created with the number of inputs and outputs specified in the 'sz' list\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "    # Define the forward pass of the MLP\n",
    "    def __call__(self, x):\n",
    "        # Pass the input 'x' through each layer in the MLP\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        # Return the final output after passing through all layers\n",
    "        return x\n",
    "    \n",
    "    # Gather all parameters (weights and biases) from all layers in the MLP\n",
    "    def parameters(self):\n",
    "        # Flatten the list of parameters from each layer into a single list\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.5766674457872752)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the input vector 'x' with 3 elements\n",
    "x = [2.0, 3.0, -1.0]\n",
    "\n",
    "# Create an instance of the MLP (Multi-Layer Perceptron) class\n",
    "# The MLP has 3 input neurons, two hidden layers with 4 neurons each, and 1 output neuron\n",
    "n = MLP(3, [4, 4, 1])\n",
    "\n",
    "# Perform a forward pass of the MLP with the input vector 'x'\n",
    "# This will compute the output of the MLP based on the current weights and biases\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.1986388682369966),\n",
       " Value(data=-0.38658409972450913),\n",
       " Value(data=-0.43356535849638655),\n",
       " Value(data=0.5283833703018346),\n",
       " Value(data=0.40947882159933346),\n",
       " Value(data=-0.03544273206091342),\n",
       " Value(data=-0.8675990444069803),\n",
       " Value(data=0.13285281424044548),\n",
       " Value(data=0.9701868959297075),\n",
       " Value(data=-0.287554470155327),\n",
       " Value(data=0.05578963974264184),\n",
       " Value(data=-0.1798031880524662),\n",
       " Value(data=0.9792678427998209),\n",
       " Value(data=-0.8091181672875454),\n",
       " Value(data=0.716818927344054),\n",
       " Value(data=0.5761908449845983),\n",
       " Value(data=0.5626936883683891),\n",
       " Value(data=-0.13743583850914276),\n",
       " Value(data=0.47902489980249996),\n",
       " Value(data=-0.4736473484596744),\n",
       " Value(data=-0.7059417717912215),\n",
       " Value(data=-0.5277083367358057),\n",
       " Value(data=-0.0552780165803457),\n",
       " Value(data=-0.04081743681524319),\n",
       " Value(data=-0.6845945830647193),\n",
       " Value(data=0.39300400993246476),\n",
       " Value(data=0.27208238625568004),\n",
       " Value(data=0.9712476303988329),\n",
       " Value(data=-0.24203629498725898),\n",
       " Value(data=-0.4243151304626711),\n",
       " Value(data=0.4489286706539952),\n",
       " Value(data=0.8342732415642096),\n",
       " Value(data=-0.7341968481456098),\n",
       " Value(data=0.4392615090470382),\n",
       " Value(data=-0.40717551522263395),\n",
       " Value(data=0.8854151481426686),\n",
       " Value(data=0.6689305415331497),\n",
       " Value(data=0.9187733992769147),\n",
       " Value(data=-0.6476182321938029),\n",
       " Value(data=0.18819908311153344),\n",
       " Value(data=-0.5973320389534416)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all the weights and biases in the neural network\n",
    "# This will return a list of all parameters (weights and biases) from all layers in the MLP\n",
    "# Each parameter is an instance of the Value class, which holds the data and gradient\n",
    "n.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset with 4 possible inputs into the neural net and 4 desired targets\n",
    "\n",
    "# Input data: a list of 4 input vectors, each with 3 features\n",
    "xs = [\n",
    "    [2.0, 3.0, -1.0],  # First input vector\n",
    "    [3.0, -1.0, 0.5],  # Second input vector\n",
    "    [0.5, 1.0, 1.0],   # Third input vector\n",
    "    [1.0, 1.0, -1.0]   # Fourth input vector\n",
    "]\n",
    "\n",
    "# Desired target outputs for the input data\n",
    "# This is a simple binary classifier neural net\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # Desired targets: \n",
    "                             # We want the neural net to output 1.0 for xs[0], \n",
    "                             # -1.0 for xs[1], -1.0 for xs[2], and 1.0 for xs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.038323437096592\n",
      "1 1.6320927822339133\n",
      "2 0.5678337462970057\n",
      "3 0.31243055577866463\n",
      "4 0.23414969663252577\n",
      "5 0.1869182019364849\n",
      "6 0.15520087153140555\n",
      "7 0.1324167122719434\n",
      "8 0.1152641607176135\n",
      "9 0.10189526400913762\n",
      "10 0.09119193942471443\n",
      "11 0.0824369740847832\n",
      "12 0.07514880699761076\n",
      "13 0.06899200280236892\n",
      "14 0.06372573314320923\n",
      "15 0.05917264577935922\n",
      "16 0.055199275352608525\n",
      "17 0.05170328784115038\n",
      "18 0.04860492785402208\n",
      "19 0.04584113646074944\n",
      "20 0.04336141468892814\n",
      "21 0.04112485685979385\n",
      "22 0.03909798532684644\n",
      "23 0.037253145107249815\n",
      "24 0.035567296636412234\n",
      "25 0.03402109616215525\n",
      "26 0.032598186983783466\n",
      "27 0.03128464729811663\n",
      "28 0.030068555782651\n",
      "29 0.02893964668428659\n",
      "30 0.027889033654469344\n",
      "31 0.026908986891383813\n",
      "32 0.025992751984501218\n",
      "33 0.02513440165302714\n",
      "34 0.024328713630809833\n",
      "35 0.02357106948466338\n",
      "36 0.022857370306139248\n",
      "37 0.022183966090943672\n",
      "38 0.02154759628839608\n",
      "39 0.02094533951806303\n"
     ]
    }
   ],
   "source": [
    "# The goal is to adjust the weights of the neural network to achieve the desired outputs.\n",
    "# In deep learning, we calculate a single number called the 'loss' to measure the performance of the neural network.\n",
    "# Initially, the loss is high because the neural network is not performing well.\n",
    "# We will iterate 40 times to improve the network's performance.\n",
    "for k in range(40):\n",
    "    # Forward pass: Compute the predicted outputs for each input vector in 'xs'.\n",
    "    ypred = [n(x) for x in xs]\n",
    "    \n",
    "    # Calculate the loss: This is the sum of squared differences between predicted and target outputs.\n",
    "    # The loss measures how far off the predictions are from the actual targets.\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "    \n",
    "    # Backward pass: Reset gradients to zero before computing new gradients.\n",
    "    for p in n.parameters():\n",
    "        p.grad = 0.0\n",
    "    # Compute gradients of the loss with respect to all parameters.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters: Adjust each parameter by moving in the direction that reduces the loss.\n",
    "    # The learning rate is 0.05, which controls the size of the update step.\n",
    "    for p in n.parameters():\n",
    "        p.data += -0.05 * p.grad\n",
    "    \n",
    "    # Print the current iteration number and the loss value to monitor progress.\n",
    "    print(k, loss.data)\n",
    "\n",
    "# For each of the 4 examples, we calculate the squared difference between the prediction and the ground truth.\n",
    "# If the prediction is close to the target, the squared difference is small, indicating a good performance.\n",
    "# Squaring ensures that the loss is always positive, regardless of whether the prediction is above or below the target.\n",
    "# The more the prediction deviates from the target, the higher the loss.\n",
    "# Our goal is to minimize the loss, indicating better performance of the neural network.\n",
    "# The final loss is the sum of all these squared differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.9460600613150587),\n",
       " Value(data=-0.9204856875534216),\n",
       " Value(data=-0.9350846831651825),\n",
       " Value(data=0.9134015110460294)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'ypred' contains the predicted outputs of the neural network for each input vector in 'xs'.\n",
    "# These predictions are the result of the forward pass through the network.\n",
    "# Each element in 'ypred' corresponds to the network's output for a specific input vector.\n",
    "# The goal is for these predictions to be as close as possible to the desired target outputs 'ys'.\n",
    "ypred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of What We Have Learned\n",
    "\n",
    "### What Are Neural Nets?\n",
    "\n",
    "Neural networks are mathematical expressions that:\n",
    "\n",
    "1. **Take input as data**:\n",
    "   - They accept data inputs and also take the weights and parameters of the neuron.\n",
    "\n",
    "2. **Perform a forward pass**:\n",
    "   - This involves calculating the neuron’s mathematical expression to predict outputs.\n",
    "   - A loss function then measures the accuracy of the predictions. Generally, the loss will be low when predictions closely match the targets, indicating that the network is performing well.\n",
    "\n",
    "3. **Optimize using the loss function**:\n",
    "   - The goal is to manipulate the loss function so that when the loss is low, the network solves the problem effectively.\n",
    "\n",
    "4. **Backpropagate the loss**:\n",
    "   - Through backpropagation, the gradient of the loss is calculated, which informs how to tune all the parameters to decrease the loss locally.\n",
    "   - This process is iterated multiple times using gradient descent.\n",
    "\n",
    "5. **Minimize the loss**:\n",
    "   - By following the gradient, the loss is minimized, ensuring that the network performs the desired task correctly.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- Neural networks can be seen as a \"blob\" of neural connections that can perform arbitrary tasks, which is the source of their power.\n",
    "- Even a tiny network with 41 parameters can solve problems, but significantly more complex neural networks with billions (or even trillions) of parameters are now common.\n",
    "\n",
    "### Complex Neural Nets and Emerging Properties\n",
    "\n",
    "- **Example: GPT**\n",
    "  - In the case of GPT, the task involves predicting the next word in a sequence based on massive amounts of internet text data. This learning problem uses:\n",
    "    - A neural network with hundreds of billions of parameters.\n",
    "    - Cross-entropy loss instead of mean squared error for predicting the next token.\n",
    "  - Despite its complexity, the underlying principles remain identical:\n",
    "    - Gradient evaluation is the same.\n",
    "    - Gradient descent operates similarly.\n",
    "\n",
    "- **Remarkable Emerging Properties**:\n",
    "  - Training on large datasets often reveals fascinating and unexpected behaviors in neural networks.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Neural network setup and training, regardless of scale, fundamentally operate on the same principles. Now, there is an intuitive understanding of how these processes work under the hood, making it easier to grasp the capabilities of neural networks for solving extremely complex problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
